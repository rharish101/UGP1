\documentclass[12pt,a4paper,twocolumn]{article}
\usepackage[margin=20mm,bottom=30mm,footskip=15mm]{geometry}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{cite}

\titleformat{\section}{\Large\scshape\centering}{\romannumeral\thesection.}{3mm}{}
\setlength{\columnsep}{6mm}

\begin{document}
    \begin{titlepage}
        \centering

        \vspace*{2cm}
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.3\textwidth]{iitk-logo.png}\\[15mm]
        \end{figure}
        \textsc{\LARGE Indian Institute of Technology Kanpur}\\[1cm]
        {\Large CS395A}\\[7mm]

        \hrule
        \vspace{3mm}
        \textbf{\Large Undergraduate Project - 1}\\[5mm]
        \hrule
        \vspace{3mm}

        \begin{multicols}{2}
            \textit{Students:}\\[2mm]
            Harish Rajagopal (160552)\\
            Vishwas Lathi (160808)\\

            \vfill\null\columnbreak

            \textit{Professor:}\\[2mm]
            Vinay Namboodiri\\
            Dept. of Computer Science and Engineering
        \end{multicols}
    \end{titlepage}

    \twocolumn[
        \centering\LARGE Project Report
        \vspace{15mm}
    ]

    \section{Abstract}
        We propose MAD-SRGAN which is multi-agent generalization to \cite{SRGAN}.
        SRGAN, a generative adversarial network for image super-resolution is capable of generating photo-realistic natural $4\times$ upscaled images.
        The authors of SRGAN proposed a perceptual loss function which consists of adversarial loss and content loss.

        SRGAN consists of a single discriminator which tries to discriminate whether generated images are realistic or not, while the generator tries to push the generated image towards the manifold of the natural images.
        MADGAN \cite{madgan} is a generalization of GAN to address the problem of mode collapse, which most of the GAN's and their variants suffer from.
        We propose a modification to vanilla SRGAN by combining MADGAN and SRGAN, namely MAD-SRGAN which consists of multiple generators and further variants also consists of multiple discriminators.

    \section{MAD-SRGAN}
        MAD-SRGAN consists of a $4$ mini-generators each of which which deep residual network (ResNet) with skip connections.
        Each residual block consists of  two convolution layers with $3\times3$ kernel and $64$ filters followed by batch-normalization layer and $ReLu$ as the activation function.

        To discriminate whether the generated images lie close to natural images manifold, we have a discriminator network.
        The network uses the architecture proposed by Radford et al.
        It consists of $8$ convolution layers with $3\times3$ filter kernels.
        The feature maps obtained from the convolution layers is fed as input to two densely connected layers, which outputs the probability of classification using a sigmoid function.
        The network is trained using high-resolution images which are $4\times$ downscaled and then fed as input to the network while the ground truth remains the upscaled images.

        The input image is divided into $4$ equal and slightly overlapping sections, each corresponding the top-left, top-right, bottom-left and bottom-right portion of the input image.
        Each section is then fed to a mini-generator which tries to generate the corresponding section in the high resolution target image.
        The output of all the generators is concatenated in same fashion as the input image was divided into sections.
        This is then passed as an input to the discriminator which tries to identify whether input image is fake or not.

        The loss function $L$ consists of three parts, namely the content loss $(L_{content})$, adversarial loss $(L_{adv})$ and the overlap loss $(L_{overlap})$.\\
        $$L = L_{content} + \alpha L_{adv} + \beta L_{overlap}$$ where $\alpha$ and $\beta$ are hyperparameters.

        !!NEED TO DESCRIBE THE CONTENT AND ADV. LOSS


    % Lorem ipsum dolor sit amet\cite{srgan}, consectetur ad ipiscing elit\cite{tensorflow}. Praesent quis bibendum quam. Vivamus finibus non sapien in pellentesque. Mauris auctor a dui id pulvinar. In hac habitasse platea dictumst. Ut a viverra est, eu eleifend massa. Maecenas auctor varius massa, eu interdum quam suscipit sit amet. Pellentesque lobortis imperdiet ex, quis rhoncus nunc cursus ac. Quisque interdum leo in lacinia pellentesque. Praesent interdum rutrum elit, elementum vestibulum lacus mattis ut. Duis consectetur, ex quis efficitur varius, nulla mi imperdiet nisi, et fringilla nisi urna ut lacus.

    % Cras ac quam sodales nisi molestie pretium et sed mi. Vivamus eleifend vitae arcu eu maximus. Vivamus hendrerit eleifend placerat. In nec justo augue. Cras dapibus risus volutpat diam porta, at fermentum purus laoreet. Nam sagittis vitae dui at volutpat. Nunc ullamcorper iaculis justo, vel vestibulum tortor aliquam ac. Vestibulum sed leo purus. Proin at nisl nibh. Phasellus sed scelerisque felis. Etiam ullamcorper quis tellus semper lobortis.

    % Fusce at mauris euismod, faucibus risus sit amet, tempor urna. In hac habitasse platea dictumst. Nulla vel sollicitudin risus. Duis nulla est, condimentum eget eleifend id, mattis et orci. Sed placerat orci sit amet porttitor imperdiet. Morbi in magna in ex molestie porttitor quis a elit. Morbi enim quam, sodales sit amet aliquet elementum, volutpat at sapien. Aliquam in felis non ante vulputate interdum. Duis a cursus nisi. Suspendisse tristique nisl ut tincidunt mollis. Sed tincidunt tortor leo, in bibendum tellus commodo finibus. Phasellus et risus nisl. Etiam mattis neque ligula, ullamcorper vehicula leo tincidunt non. Fusce turpis dui, scelerisque nec vestibulum quis, fringilla tempus libero. Morbi lacus turpis, malesuada at felis ac, tincidunt egestas turpis.

    % In a est quis lectus tempor molestie. In condimentum suscipit faucibus. Ut fermentum erat non lacus lacinia, in porttitor nisi feugiat. Sed sed fermentum purus, sit amet faucibus ex. Etiam finibus metus massa, sed varius quam vestibulum sit amet. Maecenas blandit mi vitae turpis lobortis tempus. Proin sit amet lacus lobortis, lobortis orci a, lobortis erat. Nullam eleifend ante in tempor viverra. Donec efficitur, justo non ultrices molestie, tellus ipsum tempus eros, vel venenatis metus erat non ipsum. Nam massa tellus, accumsan at mauris vel, molestie efficitur nulla.

    \bibliographystyle{ieeetr}
    \bibliography{refer}
\end{document}
